{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1a2a97",
   "metadata": {},
   "source": [
    "# Code to Medium Article Generator with Persistent Vector Store\n",
    "\n",
    "This notebook creates a system that:\n",
    "1. Reads code files from a specified folder\n",
    "2. Persistently stores their content in a Chroma vector store\n",
    "3. Generates technical Medium articles through a Gradio chat interface\n",
    "4. Allows resuming processing from previous state\n",
    "5. Formats articles in active voice, targeting recruiters and managers\n",
    "\n",
    "Key Features:\n",
    "- Persistent vector store using SQLite backend\n",
    "- Progress tracking and state management\n",
    "- Chunk-based processing with automatic saves\n",
    "- Resume capability for interrupted processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800d875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: chromadb in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: pypdf in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (5.8.0)\n",
      "Requirement already satisfied: gradio in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (5.38.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (3.11.0)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.11.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (1.11.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.34.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.47.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kstvk\\anaconda3\\envs\\ai-engineering\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-community chromadb pypdf gradio requests numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4f7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kstvk\\anaconda3\\envs\\ai-engineering\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd38ae8",
   "metadata": {},
   "source": [
    "# Vector Store Configuration and Helper Functions\n",
    "\n",
    "We'll set up:\n",
    "1. Directory configuration for persistent storage\n",
    "2. Helper functions for time formatting and progress tracking\n",
    "3. Vector store initialization and management functions\n",
    "4. Processing progress tracking and state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fed7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for time formatting\n",
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds to human readable time format\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.1f} minutes\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.1f} hours\"\n",
    "\n",
    "# Vector store configuration\n",
    "PERSIST_DIRECTORY = os.path.join(os.getcwd(), \"vector_store\")\n",
    "CODE_PERSIST_DIRECTORY = os.path.join(os.getcwd(), \"code_store\")\n",
    "PROGRESS_FILE = os.path.join(PERSIST_DIRECTORY, \"processing_progress.json\")\n",
    "CODE_PROGRESS_FILE = os.path.join(CODE_PERSIST_DIRECTORY, \"code_progress.json\")\n",
    "\n",
    "# Supported code file extensions\n",
    "CODE_EXTENSIONS = {\n",
    "    'python': ['.py'],\n",
    "    'javascript': ['.js', '.jsx', '.ts', '.tsx'],\n",
    "    'java': ['.java'],\n",
    "    'csharp': ['.cs']\n",
    "}\n",
    "\n",
    "# Helper function for time formatting\n",
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds to human readable time format\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.1f} minutes\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.1f} hours\"\n",
    "\n",
    "# Vector store configuration\n",
    "PERSIST_DIRECTORY = os.path.join(os.getcwd(), \"vector_store\")\n",
    "PROGRESS_FILE = os.path.join(PERSIST_DIRECTORY, \"processing_progress.json\")\n",
    "\n",
    "def verify_ollama_connection():\n",
    "    \"\"\"Verify that Ollama is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Ollama connection verified\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ Ollama is not responding correctly\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"✗ Could not connect to Ollama. Is it running?\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def test_embeddings():\n",
    "    \"\"\"Test that embeddings are working correctly.\"\"\"\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "        test_text = \"This is a test sentence.\"\n",
    "        result = embeddings.embed_query(test_text)\n",
    "        \n",
    "        if isinstance(result, list) and len(result) > 0 and all(isinstance(x, float) for x in result):\n",
    "            print(\"✓ Embeddings test successful\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ Invalid embedding output format\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(\"✗ Embeddings test failed\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_processing_progress(processed_files):\n",
    "    \"\"\"Save the list of processed files and their hashes.\"\"\"\n",
    "    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(processed_files, f)\n",
    "\n",
    "def load_processing_progress():\n",
    "    \"\"\"Load the list of previously processed files.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def get_file_hash(file_path):\n",
    "    \"\"\"Calculate hash of file for tracking changes.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def initialize_vector_store():\n",
    "    \"\"\"Initialize or load existing vector store.\"\"\"\n",
    "    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    \n",
    "    if os.path.exists(os.path.join(PERSIST_DIRECTORY, \"chroma.sqlite3\")):\n",
    "        print(\"Loading existing vector store...\")\n",
    "        return Chroma(\n",
    "            persist_directory=PERSIST_DIRECTORY,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating new vector store...\")\n",
    "        return Chroma(\n",
    "            persist_directory=PERSIST_DIRECTORY,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "def save_vector_store(vectorstore):\n",
    "    \"\"\"Save vector store to disk.\"\"\"\n",
    "    print(\"Saving vector store...\")\n",
    "    vectorstore.persist()\n",
    "    print(f\"Vector store saved to {PERSIST_DIRECTORY}\")\n",
    "\n",
    "def clear_vector_store():\n",
    "    \"\"\"Clear the vector store and processing history.\"\"\"\n",
    "    import shutil\n",
    "    if os.path.exists(PERSIST_DIRECTORY):\n",
    "        shutil.rmtree(PERSIST_DIRECTORY)\n",
    "        os.makedirs(PERSIST_DIRECTORY)\n",
    "        print(\"Vector store cleared successfully\")\n",
    "\n",
    "def get_vector_store_stats():\n",
    "    \"\"\"Get statistics about the vector store.\"\"\"\n",
    "    if os.path.exists(os.path.join(PERSIST_DIRECTORY, \"chroma.sqlite3\")):\n",
    "        vectorstore = initialize_vector_store()\n",
    "        collection = vectorstore._collection\n",
    "        stats = {\n",
    "            \"total_documents\": collection.count(),\n",
    "            \"persist_directory\": PERSIST_DIRECTORY,\n",
    "            \"processed_files\": len(load_processing_progress())\n",
    "        }\n",
    "        return stats\n",
    "    return {\n",
    "        \"total_documents\": 0, \n",
    "        \"persist_directory\": PERSIST_DIRECTORY, \n",
    "        \"processed_files\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e896e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_code_store():\n",
    "    \"\"\"Initialize or load existing code vector store.\"\"\"\n",
    "    os.makedirs(CODE_PERSIST_DIRECTORY, exist_ok=True)\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    \n",
    "    if os.path.exists(os.path.join(CODE_PERSIST_DIRECTORY, \"chroma.sqlite3\")):\n",
    "        print(\"Loading existing code vector store...\")\n",
    "        return Chroma(\n",
    "            persist_directory=CODE_PERSIST_DIRECTORY,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating new code vector store...\")\n",
    "        return Chroma(\n",
    "            persist_directory=CODE_PERSIST_DIRECTORY,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "def save_code_progress(processed_files):\n",
    "    \"\"\"Save the list of processed code files and their hashes.\"\"\"\n",
    "    os.makedirs(CODE_PERSIST_DIRECTORY, exist_ok=True)\n",
    "    with open(CODE_PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(processed_files, f)\n",
    "\n",
    "def load_code_progress():\n",
    "    \"\"\"Load the list of previously processed code files.\"\"\"\n",
    "    if os.path.exists(CODE_PROGRESS_FILE):\n",
    "        with open(CODE_PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def clear_code_store():\n",
    "    \"\"\"Clear the code vector store and processing history.\"\"\"\n",
    "    import shutil\n",
    "    if os.path.exists(CODE_PERSIST_DIRECTORY):\n",
    "        shutil.rmtree(CODE_PERSIST_DIRECTORY)\n",
    "        os.makedirs(CODE_PERSIST_DIRECTORY)\n",
    "        print(\"Code vector store cleared successfully\")\n",
    "\n",
    "def get_code_store_stats():\n",
    "    \"\"\"Get statistics about the code vector store.\"\"\"\n",
    "    if os.path.exists(os.path.join(CODE_PERSIST_DIRECTORY, \"chroma.sqlite3\")):\n",
    "        vectorstore = initialize_code_store()\n",
    "        collection = vectorstore._collection\n",
    "        stats = {\n",
    "            \"total_documents\": collection.count(),\n",
    "            \"persist_directory\": CODE_PERSIST_DIRECTORY,\n",
    "            \"processed_files\": len(load_code_progress())\n",
    "        }\n",
    "        return stats\n",
    "    return {\n",
    "        \"total_documents\": 0, \n",
    "        \"persist_directory\": CODE_PERSIST_DIRECTORY, \n",
    "        \"processed_files\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a746b7",
   "metadata": {},
   "source": [
    "# Document Processing and Vector Store Population\n",
    "\n",
    "Now we'll implement the core document processing functionality:\n",
    "1. Load PDFs from the specified directory\n",
    "2. Split documents into chunks\n",
    "3. Process and store embeddings with progress tracking\n",
    "4. Save state at regular intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0cabdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code_files(directory):\n",
    "    \"\"\"Load all code files from the specified directory recursively.\"\"\"\n",
    "    code_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = Path(os.path.join(root, file))\n",
    "            file_ext = file_path.suffix.lower()\n",
    "            \n",
    "            # Check if file extension is in supported extensions\n",
    "            for lang_exts in CODE_EXTENSIONS.values():\n",
    "                if file_ext in lang_exts:\n",
    "                    code_files.append(file_path)\n",
    "                    break\n",
    "    \n",
    "    if not code_files:\n",
    "        print(\"No supported code files found in the specified directory.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nLoading {len(code_files)} code files...\")\n",
    "    documents = []\n",
    "    \n",
    "    for file_path in code_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                # Create a document with metadata\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        'source': str(file_path),\n",
    "                        'file_type': file_path.suffix.lower(),\n",
    "                        'file_name': file_path.name\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                print(f\"✓ Loaded {file_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def process_code_files(documents, vectorstore=None):\n",
    "    \"\"\"Process code files and store in the code vector store.\"\"\"\n",
    "    if not documents:\n",
    "        print(\"No code documents to process.\")\n",
    "        return initialize_code_store()\n",
    "    \n",
    "    # Verify Ollama connection and embeddings before processing\n",
    "    if not verify_ollama_connection() or not test_embeddings():\n",
    "        raise RuntimeError(\"Failed to initialize Ollama and embeddings. Please check the error messages above.\")\n",
    "    \n",
    "    # Initialize or load vector store\n",
    "    if vectorstore is None:\n",
    "        vectorstore = initialize_code_store()\n",
    "    \n",
    "    # Load processing progress\n",
    "    processed_files = load_code_progress()\n",
    "    \n",
    "    # Initialize timing and progress tracking\n",
    "    start_time = time.time()\n",
    "    total_docs = len(documents)\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting processing of {total_docs} code files\")\n",
    "    \n",
    "    # Text splitter optimized for code\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,  # Larger chunks for code to maintain context\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"    \", \"\\t\"]\n",
    "    )\n",
    "    \n",
    "    # Process documents with progress tracking\n",
    "    processed_chunks = 0\n",
    "    save_interval = 20  # Save every 20 chunks\n",
    "    last_saved_chunk = 0\n",
    "    \n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        # Check if document was already processed\n",
    "        if doc.metadata.get('source') in processed_files:\n",
    "            print(f\"Skipping already processed file: {doc.metadata.get('source')}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing file {i}/{total_docs}: {doc.metadata.get('source', 'Unknown Source')}\")\n",
    "        chunk_start = time.time()\n",
    "        texts = text_splitter.split_documents([doc])\n",
    "        processed_chunks += len(texts)\n",
    "        \n",
    "        try:\n",
    "            # Add to vector store with retry logic\n",
    "            vectorstore.add_documents(texts)\n",
    "            \n",
    "            # Save progress periodically\n",
    "            if processed_chunks > last_saved_chunk + save_interval:\n",
    "                vectorstore.persist()\n",
    "                print(f\"\\nProgress saved. Total chunks processed: {processed_chunks}\")\n",
    "                last_saved_chunk = processed_chunks\n",
    "            \n",
    "            # Update progress tracking\n",
    "            processed_files[doc.metadata.get('source')] = get_file_hash(doc.metadata.get('source'))\n",
    "            save_code_progress(processed_files)\n",
    "            \n",
    "            # Calculate and show progress\n",
    "            chunk_time = time.time() - chunk_start\n",
    "            print(f\"Chunks in this file: {len(texts)}\")\n",
    "            print(f\"Time for this file: {format_time(chunk_time)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR processing file {i}: {str(e)}\")\n",
    "            print(\"Continuing with next file...\")\n",
    "            continue\n",
    "    \n",
    "    # Final save\n",
    "    vectorstore.persist()\n",
    "    save_code_progress(processed_files)\n",
    "    \n",
    "    # Final statistics\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Processing completed:\")\n",
    "    print(f\"Total files processed: {total_docs}\")\n",
    "    print(f\"Total chunks created: {processed_chunks}\")\n",
    "    print(f\"Total processing time: {format_time(total_time)}\")\n",
    "    \n",
    "    # Show vector store stats\n",
    "    stats = get_code_store_stats()\n",
    "    print(\"\\nCode Vector Store Statistics:\")\n",
    "    print(f\"Total documents in store: {stats['total_documents']}\")\n",
    "    print(f\"Total processed files: {stats['processed_files']}\")\n",
    "    print(f\"Store location: {stats['persist_directory']}\")\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94daa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(directory):\n",
    "    \"\"\"Load all PDFs from the specified directory.\"\"\"\n",
    "    pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nLoading {len(pdf_files)} PDF files...\")\n",
    "    documents = []\n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_path))\n",
    "            documents.extend(loader.load())\n",
    "            print(f\"✓ Loaded {pdf_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {pdf_path.name}: {str(e)}\")\n",
    "    return documents\n",
    "\n",
    "def process_documents(documents, vectorstore=None):\n",
    "    \"\"\"Split documents and create/update vector store with progress tracking.\"\"\"\n",
    "    if not documents:\n",
    "        print(\"No documents to process.\")\n",
    "        return initialize_vector_store()\n",
    "    \n",
    "    # Verify Ollama connection and embeddings before processing\n",
    "    if not verify_ollama_connection() or not test_embeddings():\n",
    "        raise RuntimeError(\"Failed to initialize Ollama and embeddings. Please check the error messages above.\")\n",
    "    \n",
    "    # Initialize or load vector store\n",
    "    if vectorstore is None:\n",
    "        vectorstore = initialize_vector_store()\n",
    "    \n",
    "    # Load processing progress\n",
    "    processed_files = load_processing_progress()\n",
    "    \n",
    "    # Initialize timing and progress tracking\n",
    "    start_time = time.time()\n",
    "    total_docs = len(documents)\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting processing of {total_docs} documents\")\n",
    "    \n",
    "    # Text splitter with optimized chunk size\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Estimate chunks and time\n",
    "    sample_size = min(5, len(documents))\n",
    "    sample_chunks = sum(len(text_splitter.split_text(doc.page_content)) for doc in documents[:sample_size])\n",
    "    estimated_total_chunks = math.ceil((sample_chunks / sample_size) * len(documents))\n",
    "    print(f\"Estimated total chunks: {estimated_total_chunks} (based on {sample_size} document sample)\")\n",
    "    \n",
    "    # Process documents with progress tracking\n",
    "    processed_chunks = 0\n",
    "    save_interval = 30  # Save every 30 chunks for better persistence\n",
    "    last_saved_chunk = 0\n",
    "    \n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        # Check if document was already processed\n",
    "        if doc.metadata.get('source') in processed_files:\n",
    "            print(f\"Skipping already processed document: {doc.metadata.get('source')}\")\n",
    "            continue\n",
    "        print(f\"\\nProcessing document {i}/{total_docs}: {doc.metadata.get('source', 'Unknown Source')}\")\n",
    "        chunk_start = time.time()\n",
    "        texts = text_splitter.split_documents([doc])\n",
    "        processed_chunks += len(texts)\n",
    "        \n",
    "        try:\n",
    "            # Add to vector store with retry logic\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    vectorstore.add_documents(texts)\n",
    "                    \n",
    "                    # Save progress periodically\n",
    "                    if processed_chunks > last_saved_chunk + save_interval:\n",
    "                        save_vector_store(vectorstore)\n",
    "                        print(f\"\\nProgress saved. Total chunks processed: {processed_chunks}\")\n",
    "                        last_saved_chunk = processed_chunks\n",
    "                    \n",
    "                    break\n",
    "                except ValueError as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise\n",
    "                    print(f\"Retry {attempt + 1}/{max_retries} - Waiting 2 seconds before retry...\")\n",
    "                    time.sleep(2)\n",
    "        \n",
    "            # Calculate progress and estimates\n",
    "            chunk_time = time.time() - chunk_start\n",
    "            elapsed_total = time.time() - start_time\n",
    "            avg_time_per_chunk = elapsed_total / processed_chunks\n",
    "            estimated_remaining_chunks = estimated_total_chunks - processed_chunks\n",
    "            estimated_remaining_time = estimated_remaining_chunks * avg_time_per_chunk\n",
    "            \n",
    "            print(f\"\\nProgress: Document {i}/{total_docs}\")\n",
    "            print(f\"Chunks in this document: {len(texts)}\")\n",
    "            print(f\"Total chunks processed: {processed_chunks}/{estimated_total_chunks}\")\n",
    "            print(f\"Time for this document: {format_time(chunk_time)}\")\n",
    "            print(f\"Estimated remaining time: {format_time(estimated_remaining_time)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR processing document {i}: {str(e)}\")\n",
    "            print(\"Continuing with next document...\")\n",
    "            continue\n",
    "    \n",
    "    # Final save\n",
    "    save_vector_store(vectorstore)\n",
    "    processed_files[doc.metadata.get('source')] = get_file_hash(doc.metadata.get('source'))\n",
    "    save_processing_progress(processed_files)\n",
    "    \n",
    "    # Final statistics\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Processing completed:\")\n",
    "    print(f\"Total documents processed: {total_docs}\")\n",
    "    print(f\"Total chunks created: {processed_chunks}\")\n",
    "    print(f\"Total processing time: {format_time(total_time)}\")\n",
    "    print(f\"Average time per chunk: {(total_time/processed_chunks):.2f} seconds\")\n",
    "    \n",
    "    # Show vector store stats\n",
    "    stats = get_vector_store_stats()\n",
    "    print(\"\\nVector Store Statistics:\")\n",
    "    print(f\"Total documents in store: {stats['total_documents']}\")\n",
    "    print(f\"Total processed files: {stats['processed_files']}\")\n",
    "    print(f\"Store location: {stats['persist_directory']}\")\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9025a94",
   "metadata": {},
   "source": [
    "# Article Generation with LLM Chain\n",
    "\n",
    "We'll set up the article generation pipeline:\n",
    "1. Create a prompt template for technical articles\n",
    "2. Initialize the LLM chain with Ollama\n",
    "3. Create a function to generate articles from vector store content\n",
    "4. Add a Gradio interface for easy interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e8074f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the article generation chain\n",
    "article_template = \"\"\"\n",
    "You are an expert technical writer and software engineer creating a Medium article to showcase your skills in AI and ML.\n",
    "Topic: {topic}\n",
    "Retrieved Content:\n",
    "PDF Content: {pdf_context}\n",
    "Code Content: {code_context}\n",
    "\n",
    "Write a 1200-word technical article that:\n",
    "1. Uses active voice throughout\n",
    "2. Explains technical concepts in business-friendly language\n",
    "3. Highlights practical applications and business value\n",
    "4. Includes relevant examples from both the documentation and code\n",
    "5. Structures content with clear headings and subheadings\n",
    "\n",
    "Article:\n",
    "\"\"\"\n",
    "\n",
    "article_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"pdf_context\", \"code_context\"],\n",
    "    template=article_template\n",
    ")\n",
    "\n",
    "pdf_directory = \"pdfs123\"\n",
    "code_directory = \"code\"\n",
    "\n",
    "# Initialize Ollama with llama3 model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "article_chain = LLMChain(llm=llm, prompt=article_prompt)\n",
    "\n",
    "def generate_article(topic, pdf_vectorstore, code_vectorstore):\n",
    "    \"\"\"Generate a Medium article based on the topic and both vector stores' content.\"\"\"\n",
    "    # Search both vector stores for relevant content\n",
    "    pdf_results = pdf_vectorstore.similarity_search(topic, k=3)\n",
    "    code_results = code_vectorstore.similarity_search(topic, k=3)\n",
    "    \n",
    "    pdf_context = \"\\n\".join([doc.page_content for doc in pdf_results])\n",
    "    code_context = \"\\n\".join([doc.page_content for doc in code_results])\n",
    "    \n",
    "    # Generate the article\n",
    "    article = article_chain.run(topic=topic, pdf_context=pdf_context, code_context=code_context)\n",
    "    return article\n",
    "\n",
    "def gradio_interface(topic):\n",
    "    \"\"\"Gradio interface function for article generation.\"\"\"\n",
    "    try:\n",
    "        has_pdfs = any(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "        has_code =  any(Path(code_directory).rglob(\"*.*\"))\n",
    "        \n",
    "        if not has_pdfs and not has_code:\n",
    "            return \"Error: No PDF files or code files found. Please add some files first.\"\n",
    "        \n",
    "        # Initialize vector stores\n",
    "        pdf_vectorstore = initialize_vector_store()\n",
    "        code_vectorstore = initialize_code_store()\n",
    "        \n",
    "        if not pdf_vectorstore or not code_vectorstore:\n",
    "            return \"Error: Could not initialize vector stores. Please check if the stores exist and contain documents.\"\n",
    "        \n",
    "        article = generate_article(topic, pdf_vectorstore, code_vectorstore)\n",
    "        return article\n",
    "    except Exception as e:\n",
    "        return f\"Error generating article: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2c1f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:34:26] Starting file processing pipeline\n",
      "✓ Ollama connection verified\n",
      "✓ Ollama connection verified\n",
      "✓ Embeddings test successful\n",
      "\n",
      "Current Vector Store Status:\n",
      "Loading existing vector store...\n",
      "Loading existing code vector store...\n",
      "\n",
      "PDF Vector Store:\n",
      "Total documents: 2570\n",
      "Total processed files: 1\n",
      "\n",
      "Code Vector Store:\n",
      "Total documents: 23\n",
      "Total processed files: 20\n",
      "\n",
      "No PDF files found in the 'pdfs' directory.\n",
      "\n",
      "Processing code files...\n",
      "\n",
      "Loading 20 code files...\n",
      "✓ Loaded BillingSystem.java\n",
      "✓ Loaded Driver.java\n",
      "✓ Loaded Main.java\n",
      "✓ Loaded Trip.java\n",
      "✓ Loaded Vehicle.java\n",
      "✓ Loaded Locker.java\n",
      "✓ Loaded LockerManager.java\n",
      "✓ Loaded LockerSystem.java\n",
      "✓ Loaded Package.java\n",
      "✓ Loaded Size.java\n",
      "✓ Loaded NotificationChannel.java\n",
      "✓ Loaded NotificationDispatcher.java\n",
      "✓ Loaded NotificationSystem.java\n",
      "✓ Loaded NotificationWorker.java\n",
      "✓ Loaded OrderService.java\n",
      "✓ Loaded EmailNotificationChannel.java\n",
      "✓ Loaded SmsNotificationChannel.java\n",
      "✓ Loaded WhatsAppNotificationChannel.java\n",
      "✓ Loaded NotificationEvent.java\n",
      "✓ Loaded Notification.java\n",
      "✓ Embeddings test successful\n",
      "\n",
      "Current Vector Store Status:\n",
      "Loading existing vector store...\n",
      "Loading existing code vector store...\n",
      "\n",
      "PDF Vector Store:\n",
      "Total documents: 2570\n",
      "Total processed files: 1\n",
      "\n",
      "Code Vector Store:\n",
      "Total documents: 23\n",
      "Total processed files: 20\n",
      "\n",
      "No PDF files found in the 'pdfs' directory.\n",
      "\n",
      "Processing code files...\n",
      "\n",
      "Loading 20 code files...\n",
      "✓ Loaded BillingSystem.java\n",
      "✓ Loaded Driver.java\n",
      "✓ Loaded Main.java\n",
      "✓ Loaded Trip.java\n",
      "✓ Loaded Vehicle.java\n",
      "✓ Loaded Locker.java\n",
      "✓ Loaded LockerManager.java\n",
      "✓ Loaded LockerSystem.java\n",
      "✓ Loaded Package.java\n",
      "✓ Loaded Size.java\n",
      "✓ Loaded NotificationChannel.java\n",
      "✓ Loaded NotificationDispatcher.java\n",
      "✓ Loaded NotificationSystem.java\n",
      "✓ Loaded NotificationWorker.java\n",
      "✓ Loaded OrderService.java\n",
      "✓ Loaded EmailNotificationChannel.java\n",
      "✓ Loaded SmsNotificationChannel.java\n",
      "✓ Loaded WhatsAppNotificationChannel.java\n",
      "✓ Loaded NotificationEvent.java\n",
      "✓ Loaded Notification.java\n",
      "✓ Ollama connection verified\n",
      "✓ Ollama connection verified\n",
      "✓ Embeddings test successful\n",
      "Loading existing code vector store...\n",
      "\n",
      "[21:34:36] Starting processing of 20 code files\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\BillingSystem.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Driver.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Main.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Trip.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Vehicle.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\Locker.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\LockerManager.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\LockerSystem.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\Package.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\Size.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationDispatcher.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationSystem.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationWorker.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\OrderService.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\channels\\EmailNotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\channels\\SmsNotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\channels\\WhatsAppNotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\events\\NotificationEvent.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\models\\Notification.java\n",
      "\n",
      "[21:34:36] Processing completed:\n",
      "Total files processed: 20\n",
      "Total chunks created: 0\n",
      "Total processing time: 0.0 seconds\n",
      "Loading existing code vector store...\n",
      "\n",
      "Code Vector Store Statistics:\n",
      "Total documents in store: 23\n",
      "Total processed files: 20\n",
      "Store location: c:\\Workspace\\SideProjects\\llm-projects\\TechWriter\\code_store\n",
      "[21:34:36] Code vector store creation completed\n",
      "✓ Embeddings test successful\n",
      "Loading existing code vector store...\n",
      "\n",
      "[21:34:36] Starting processing of 20 code files\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\BillingSystem.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Driver.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Main.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Trip.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\driverbillingsystem\\Vehicle.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\Locker.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\LockerManager.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\LockerSystem.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\Package.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\lockerSystem\\Size.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationDispatcher.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationSystem.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\NotificationWorker.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\OrderService.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\channels\\EmailNotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\channels\\SmsNotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\channels\\WhatsAppNotificationChannel.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\events\\NotificationEvent.java\n",
      "Skipping already processed file: code\\MachineCoding\\src\\main\\java\\scalablenotificationsystem\\models\\Notification.java\n",
      "\n",
      "[21:34:36] Processing completed:\n",
      "Total files processed: 20\n",
      "Total chunks created: 0\n",
      "Total processing time: 0.0 seconds\n",
      "Loading existing code vector store...\n",
      "\n",
      "Code Vector Store Statistics:\n",
      "Total documents in store: 23\n",
      "Total processed files: 20\n",
      "Store location: c:\\Workspace\\SideProjects\\llm-projects\\TechWriter\\code_store\n",
      "[21:34:36] Code vector store creation completed\n"
     ]
    }
   ],
   "source": [
    "# Directory setup and initial processing\n",
    "pdf_directory = \"pdfs123\"\n",
    "code_directory = \"code\"\n",
    "os.makedirs(pdf_directory, exist_ok=True)\n",
    "os.makedirs(code_directory, exist_ok=True)\n",
    "\n",
    "# Process files if they exist\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting file processing pipeline\")\n",
    "\n",
    "# Verify Ollama setup first\n",
    "if verify_ollama_connection() and test_embeddings():\n",
    "    # Show existing vector store stats if any\n",
    "    print(\"\\nCurrent Vector Store Status:\")\n",
    "    pdf_stats = get_vector_store_stats()\n",
    "    code_stats = get_code_store_stats()\n",
    "    print(\"\\nPDF Vector Store:\")\n",
    "    print(f\"Total documents: {pdf_stats['total_documents']}\")\n",
    "    print(f\"Total processed files: {pdf_stats['processed_files']}\")\n",
    "    print(\"\\nCode Vector Store:\")\n",
    "    print(f\"Total documents: {code_stats['total_documents']}\")\n",
    "    print(f\"Total processed files: {code_stats['processed_files']}\")\n",
    "    \n",
    "    # Process PDFs\n",
    "    if any(Path(pdf_directory).glob(\"*.pdf\")):\n",
    "        print(\"\\nProcessing PDF files...\")\n",
    "        documents = load_pdfs(pdf_directory)\n",
    "        vectorstore = process_documents(documents)\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] PDF vector store creation completed\")\n",
    "    else:\n",
    "        print(\"\\nNo PDF files found in the 'pdfs' directory.\")\n",
    "    \n",
    "    # Process code files\n",
    "    if any(Path(code_directory).rglob(\"*.*\")):\n",
    "        print(\"\\nProcessing code files...\")\n",
    "        code_documents = load_code_files(code_directory)\n",
    "        code_vectorstore = process_code_files(code_documents)\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Code vector store creation completed\")\n",
    "    else:\n",
    "        print(\"\\nNo code files found in the 'code' directory.\")\n",
    "else:\n",
    "    print(\"Setup verification failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e790b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n",
      "Loading existing code vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kstvk\\AppData\\Local\\Temp\\ipykernel_34396\\1255243895.py:41: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  article = article_chain.run(topic=topic, pdf_context=pdf_context, code_context=code_context)\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=2,\n",
    "        placeholder=\"Enter the topic for your technical article...\",\n",
    "        label=\"Article Topic\"\n",
    "    ),\n",
    "    outputs=gr.Markdown(\n",
    "        label=\"Generated Article\"\n",
    "    ),\n",
    "    title=\"Technical Article Generator\",\n",
    "    description=\"Generate a 1000-word technical article from your PDF content, targeted at recruiters and managers.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()  # Close the interface when done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be05f2",
   "metadata": {},
   "source": [
    "# How to Use\n",
    "\n",
    "1. Create two directories in the same location as this notebook:\n",
    "   - `pdfs` for PDF documentation files\n",
    "   - `code` for source code files\n",
    "2. Place your files in the appropriate directories:\n",
    "   - PDF files in the `pdfs` directory\n",
    "   - Code files (.py, .js, .java, .cs, etc.) in the `code` directory\n",
    "3. Install Ollama on your system and start the Ollama service\n",
    "4. Pull the llama3 model using: `ollama pull llama3`\n",
    "5. Run all cells in this notebook\n",
    "6. Use the Gradio interface to:\n",
    "   - Enter your desired article topic\n",
    "   - Click submit to generate the article\n",
    "   - Copy the generated article and paste it into Medium\n",
    "\n",
    "The system will:\n",
    "- Automatically load and process both PDF and code files\n",
    "- Store embeddings persistently in separate stores:\n",
    "  - `vector_store` for PDFs\n",
    "  - `code_store` for code files\n",
    "- Save progress periodically during processing\n",
    "- Allow resuming from previous state if interrupted\n",
    "- Generate articles using knowledge from both documentation and code\n",
    "\n",
    "Supported Code Files:\n",
    "- Python (.py)\n",
    "- JavaScript (.js, .jsx, .ts, .tsx)\n",
    "- Java (.java)\n",
    "- C# (.cs)\n",
    "\n",
    "Notes:\n",
    "- Make sure Ollama is running before using this notebook\n",
    "- Both vector stores are persistent and will be reused across sessions\n",
    "- You can clear the stores using:\n",
    "  - `clear_vector_store()` for PDFs\n",
    "  - `clear_code_store()` for code files\n",
    "- Progress tracking prevents reprocessing of already processed files\n",
    "\n",
    "Example Usage:\n",
    "```python\n",
    "# Get current store statistics\n",
    "pdf_stats = get_vector_store_stats()\n",
    "code_stats = get_code_store_stats()\n",
    "print(f\"PDFs in store: {pdf_stats['total_documents']}\")\n",
    "print(f\"Code files in store: {code_stats['total_documents']}\")\n",
    "\n",
    "# Clear stores and start fresh\n",
    "clear_vector_store()\n",
    "clear_code_store()\n",
    "\n",
    "# Process new documents\n",
    "pdf_documents = load_pdfs(pdf_directory)\n",
    "code_documents = load_code_files(code_directory)\n",
    "\n",
    "vectorstore = process_documents(pdf_documents)\n",
    "code_vectorstore = process_code_files(code_documents)\n",
    "\n",
    "# Save explicitly if needed\n",
    "save_vector_store(vectorstore)\n",
    "code_vectorstore.persist()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
