{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1a2a97",
   "metadata": {},
   "source": [
    "# PDF to Medium Article Generator with Persistent Vector Store\n",
    "\n",
    "This notebook creates a system that:\n",
    "1. Reads PDF files from a specified folder\n",
    "2. Persistently stores their content in a Chroma vector store\n",
    "3. Generates technical Medium articles through a Gradio chat interface\n",
    "4. Allows resuming processing from previous state\n",
    "5. Formats articles in active voice, targeting recruiters and managers\n",
    "\n",
    "Key Features:\n",
    "- Persistent vector store using SQLite backend\n",
    "- Progress tracking and state management\n",
    "- Chunk-based processing with automatic saves\n",
    "- Resume capability for interrupted processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4f7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kstvk\\anaconda3\\envs\\ai-engineering\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd38ae8",
   "metadata": {},
   "source": [
    "# Vector Store Configuration and Helper Functions\n",
    "\n",
    "We'll set up:\n",
    "1. Directory configuration for persistent storage\n",
    "2. Helper functions for time formatting and progress tracking\n",
    "3. Vector store initialization and management functions\n",
    "4. Processing progress tracking and state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fed7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for time formatting\n",
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds to human readable time format\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.1f} minutes\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.1f} hours\"\n",
    "\n",
    "# Vector store configuration\n",
    "PERSIST_DIRECTORY = os.path.join(os.getcwd(), \"vector_store\")\n",
    "PROGRESS_FILE = os.path.join(PERSIST_DIRECTORY, \"processing_progress.json\")\n",
    "\n",
    "def verify_ollama_connection():\n",
    "    \"\"\"Verify that Ollama is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Ollama connection verified\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ Ollama is not responding correctly\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"✗ Could not connect to Ollama. Is it running?\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def test_embeddings():\n",
    "    \"\"\"Test that embeddings are working correctly.\"\"\"\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "        test_text = \"This is a test sentence.\"\n",
    "        result = embeddings.embed_query(test_text)\n",
    "        \n",
    "        if isinstance(result, list) and len(result) > 0 and all(isinstance(x, float) for x in result):\n",
    "            print(\"✓ Embeddings test successful\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ Invalid embedding output format\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(\"✗ Embeddings test failed\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_processing_progress(processed_files):\n",
    "    \"\"\"Save the list of processed files and their hashes.\"\"\"\n",
    "    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(processed_files, f)\n",
    "\n",
    "def load_processing_progress():\n",
    "    \"\"\"Load the list of previously processed files.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def get_file_hash(file_path):\n",
    "    \"\"\"Calculate hash of file for tracking changes.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def initialize_vector_store():\n",
    "    \"\"\"Initialize or load existing vector store.\"\"\"\n",
    "    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "    \n",
    "    if os.path.exists(os.path.join(PERSIST_DIRECTORY, \"chroma.sqlite3\")):\n",
    "        print(\"Loading existing vector store...\")\n",
    "        return Chroma(\n",
    "            persist_directory=PERSIST_DIRECTORY,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating new vector store...\")\n",
    "        return Chroma(\n",
    "            persist_directory=PERSIST_DIRECTORY,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "def save_vector_store(vectorstore):\n",
    "    \"\"\"Save vector store to disk.\"\"\"\n",
    "    print(\"Saving vector store...\")\n",
    "    vectorstore.persist()\n",
    "    print(f\"Vector store saved to {PERSIST_DIRECTORY}\")\n",
    "\n",
    "def clear_vector_store():\n",
    "    \"\"\"Clear the vector store and processing history.\"\"\"\n",
    "    import shutil\n",
    "    if os.path.exists(PERSIST_DIRECTORY):\n",
    "        shutil.rmtree(PERSIST_DIRECTORY)\n",
    "        os.makedirs(PERSIST_DIRECTORY)\n",
    "        print(\"Vector store cleared successfully\")\n",
    "\n",
    "def get_vector_store_stats():\n",
    "    \"\"\"Get statistics about the vector store.\"\"\"\n",
    "    if os.path.exists(os.path.join(PERSIST_DIRECTORY, \"chroma.sqlite3\")):\n",
    "        vectorstore = initialize_vector_store()\n",
    "        collection = vectorstore._collection\n",
    "        stats = {\n",
    "            \"total_documents\": collection.count(),\n",
    "            \"persist_directory\": PERSIST_DIRECTORY,\n",
    "            \"processed_files\": len(load_processing_progress())\n",
    "        }\n",
    "        return stats\n",
    "    return {\n",
    "        \"total_documents\": 0, \n",
    "        \"persist_directory\": PERSIST_DIRECTORY, \n",
    "        \"processed_files\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81000b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_documents': 1435,\n",
       " 'persist_directory': 'c:\\\\Workspace\\\\SideProjects\\\\llm-projects\\\\TechWriter\\\\vector_store',\n",
       " 'processed_files': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector_store_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a746b7",
   "metadata": {},
   "source": [
    "# Document Processing and Vector Store Population\n",
    "\n",
    "Now we'll implement the core document processing functionality:\n",
    "1. Load PDFs from the specified directory\n",
    "2. Split documents into chunks\n",
    "3. Process and store embeddings with progress tracking\n",
    "4. Save state at regular intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9025a94",
   "metadata": {},
   "source": [
    "# Article Generation with LLM Chain\n",
    "\n",
    "We'll set up the article generation pipeline:\n",
    "1. Create a prompt template for technical articles\n",
    "2. Initialize the LLM chain with Ollama\n",
    "3. Create a function to generate articles from vector store content\n",
    "4. Add a Gradio interface for easy interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8074f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kstvk\\AppData\\Local\\Temp\\ipykernel_8312\\1495277633.py:23: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n",
      "C:\\Users\\kstvk\\AppData\\Local\\Temp\\ipykernel_8312\\1495277633.py:24: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  article_chain = LLMChain(llm=llm, prompt=article_prompt)\n"
     ]
    }
   ],
   "source": [
    "# Setup the article generation chain\n",
    "article_template = \"\"\"\n",
    "You are an expert technical writer and software engineer creating a Medium article to showcase your skills in AI and ML.\n",
    "Topic: {topic}\n",
    "Retrieved Content: {context}\n",
    "\n",
    "Write a 1200-word technical article that:\n",
    "1. Uses active voice throughout\n",
    "2. Explains technical concepts in business-friendly language\n",
    "3. Highlights practical applications and business value\n",
    "4. Includes relevant examples from the source material\n",
    "5. Structures content with clear headings and subheadings\n",
    "\n",
    "Article:\n",
    "\"\"\"\n",
    "\n",
    "article_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"context\"],\n",
    "    template=article_template\n",
    ")\n",
    "\n",
    "# Initialize Ollama with llama3 model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "article_chain = LLMChain(llm=llm, prompt=article_prompt)\n",
    "\n",
    "def generate_article(topic, vectorstore):\n",
    "    \"\"\"Generate a Medium article based on the topic and vector store content.\"\"\"\n",
    "    # Search the vector store for relevant content\n",
    "    results = vectorstore.similarity_search(topic, k=8)\n",
    "    context = \"\\n\".join([doc.page_content for doc in results])\n",
    "    \n",
    "    # Generate the article\n",
    "    article = article_chain.run(topic=topic, context=context)\n",
    "    return article\n",
    "\n",
    "def gradio_interface(topic):\n",
    "    \"\"\"Gradio interface function for article generation.\"\"\"\n",
    "    try:        \n",
    "        # Initialize vector store\n",
    "        vectorstore = initialize_vector_store()\n",
    "        if not vectorstore:\n",
    "            return \"Error: Could not initialize vector store. Please check if the store exists and contains documents.\"\n",
    "        \n",
    "        article = generate_article(topic, vectorstore)\n",
    "        return article\n",
    "    except Exception as e:\n",
    "        return f\"Error generating article: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e790b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kstvk\\AppData\\Local\\Temp\\ipykernel_8312\\2443473163.py:71: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"llama3\")\n",
      "C:\\Users\\kstvk\\AppData\\Local\\Temp\\ipykernel_8312\\2443473163.py:75: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kstvk\\AppData\\Local\\Temp\\ipykernel_8312\\1495277633.py:33: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  article = article_chain.run(topic=topic, context=context)\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=2,\n",
    "        placeholder=\"Enter the topic for your technical article...\",\n",
    "        label=\"Article Topic\"\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        lines=20,\n",
    "        label=\"Generated Article\"\n",
    "    ),\n",
    "    title=\"Technical Article Generator\",\n",
    "    description=\"Generate a 1200-word technical article from your PDF content\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be05f2",
   "metadata": {},
   "source": [
    "# How to Use\n",
    "\n",
    "1. Create a directory named `pdfs` in the same location as this notebook\n",
    "2. Place your PDF files in the `pdfs` directory\n",
    "3. Install Ollama on your system and start the Ollama service\n",
    "4. Pull the llama3 model using: `ollama pull llama3`\n",
    "5. Run all cells in this notebook\n",
    "6. Use the Gradio interface to:\n",
    "   - Enter your desired article topic\n",
    "   - Click submit to generate the article\n",
    "   - Copy the generated article and paste it into Medium\n",
    "\n",
    "The system will:\n",
    "- Automatically load and process PDFs from the `pdfs` directory\n",
    "- Store embeddings persistently in the `vector_store` directory\n",
    "- Save progress every 30 chunks during processing\n",
    "- Allow resuming from previous state if interrupted\n",
    "- Generate articles using the stored knowledge\n",
    "\n",
    "Notes:\n",
    "- Make sure Ollama is running before using this notebook\n",
    "- The vector store is persistent and will be reused across sessions\n",
    "- You can clear the vector store using the `clear_vector_store()` function if needed\n",
    "- Progress tracking prevents reprocessing of already processed files\n",
    "\n",
    "Example Usage:\n",
    "```python\n",
    "# Get current vector store statistics\n",
    "stats = get_vector_store_stats()\n",
    "print(f\"Documents in store: {stats['total_documents']}\")\n",
    "\n",
    "# Clear vector store and start fresh\n",
    "clear_vector_store()\n",
    "\n",
    "# Process new documents\n",
    "documents = load_pdfs(pdf_directory)\n",
    "vectorstore = process_documents(documents)\n",
    "\n",
    "# Save explicitly if needed\n",
    "save_vector_store(vectorstore)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
